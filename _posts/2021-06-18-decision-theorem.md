---
layout: post
title:  결정이론
date:   2021-06-18 11:00:00 +0900
description: 분류문제와 회귀문제에서의 결정이론
categories: math
---

**결정이론(Decision theorem)**이란 새로운 값 $$x$$가 주어졌을 때 확률모델 $$p(x,t)$$에 기반해 최적의 결정을 내리는 방법을 말합니다. 이 결정이론은 크게 두 단계로 나뉩니다. 하나는 결합확률분포 $$p(x, C_k)$$를 구하는 **추론 단계**이고, 추론한 결합확률분포를 기반으로 상황에 대한 확률이 주어졌을 때 최적의 결정을 내리는 **결정 단계**입니다.

X-Ray 이미지를 이용해서 암을 판별하는 문제를 가정해 보겠습니다. 이 때 $$x$$는 X-Ray 이미지이고 $$C1, C2$$는 각각 암인 경우와 암이 아닌 경우입니다. 우리가 구하고 싶은 것은 $$p(C_k\vert x)$$의 값입니다. 이는 베이즈 확률을 통해 구할 수 있습니다.

$$p(C_k\vert x) = \frac{p(x,C_k)}{p(x)} = \frac{p(x\vert C_k)p(C_k)}{p(x)} \propto 우도(likelihood) * 사전확률(prior)$$

좋은 결정은 구하고자 하는 사후확률을 최대화시키는 $$k$$를 구하는 것입니다.

이진분류 문제에서 **결정 영역(Decision region)**은 주어진 입력을 어디에 분류할 지를 결정하는 영역을 말합니다. $$R_i=\{x: pred(x) = C_i\}$$로 나타내며, 이 때 분류를 잘못하는 분류오류 확률은 다음과 같이 구할 수 있습니다.

$$p(mis) = p(x \in R_1,c_2) + p(x \in R_2, C_1) = \int_{R1}p(x,C_2)dx + \int_{R_2}p(x,C_1)dx$$

즉 $$R_1$$을 $$C_2$$에 분류하는 확률과 $$R_2$$를 $$C_1$$에 분류하는 확률을 더한 값이 됩니다. 따라서 오류를 최소화시키려면 적절한 결정 영역을 구해야 합니다. 즉 $$p(R_1,C_1) > p(R_1,C_2)$$를 만족해야 합니다.

만약 이진분류 문제가 아닌, Multi-class 문제의 경우 정확률은 다음과 같습니다.

$$p(correct) = \sum_{k=1}^Kp(x \in R_k,C_k) = \sum_{k=1}^K\int_{R_K}p(x,C_k)dx$$

결론적으로 $$x$$가 주어졌을 때 최적의 $$k$$를 구하는 일반적인 식은 다음과 같이 정리할 수 있습니다.

$$pred(x) = argmax_kp(C_k\vert x)$$

분류문제에서 결정이론의 목표는 결합확률분포 $$p(x, C_k)$$가 주어졌을 때 최적의 결정영역들 $$R_1,R_2,...,R_k$$을 찾는 것입니다. 다른 말로는, $$x$$가 주어졌을 때 예측값 $$1,2,...,k$$을 반환하는 최적의 함수 $$\hat{C}(x)$$를 찾는 것이 됩니다. 이 때 함수 $$\hat{C}(x)$$는 예측값을 결정영역에 일대일로 매핑하는 역할을 합니다.

최적의 $$\hat{C}(x)$$를 구하기 위해서는 **손실행렬(Loss function)**을 정의해야 합니다. 손실행렬 $$L_{kj}$$은 $$C_k$$에 속하는 x를 $$C_j$$로 분류할 때 발생하는 손실을 나타냅니다. 손실행렬을 사용하는 이유는 모든 결정이 서로 다른 손실을 가지기 때문입니다. 예를 들어 암이 아닌데 암인 것으로 진단하는 오류와 암이 맞는데 암이 아닌 것으로 진단하는 오류의 손실은 다를 수밖에 없습니다.

따라서 손실행렬 $$L$$이 주어졌을 때 최적의 $$\hat{C}(x)$$는 다음과 같은 기대 손실(Expected loss)을 최소화한 것이 되는것을 알 수 있습니다.

$$E[L] = \sum_k\sum_j\int_{R_j}L_{kj}p(x,C_k)dx$$

이 식을 $$\hat{C}(x)$$에 대해 정리하면 다음과 같습니다.

$$\int \sum_{k=1}^KL_{k{\hat{C}(x)}}p(x,C_k)dx = \int \big(\sum_{k=1}^KL_{k{\hat{C}(x)}}p(C_k\vert x)\big)p(x)dx$$

따라서 함수 $$\hat{C}(x)$$는 $$E[L]$$을 최소화시킬 때 최적이 되게 됩니다.

$$\hat{C}(x) = argmin_J\sum_{k=1}^KL_kjp(C_k\vert x)$$

예를 들어, 위의 암 진단 예시에서 다음과 같이 가정하도록 하겠습니다.

$$C_K \in \{1,2\} = \{sick, healthy\}, L = \begin{vmatrix}0 & 100 \\ 1 & 0\end{vmatrix}$$

이 경우 기대손실은 다음과 같이 구할 수 있습니다.

$$E[L] = \int_{R_2}L_{1,2}p(x,C_1)dx + \int_{R1}L_{2,1}p(x,C_2)dx = \int_{R_2}100p(x,C_1)dx + \int_{R1}p(x,C_2)dx$$

따라서 이 경우, 어떤 사람이 건강하다고 판정하기 위해서는 $$p(x,C_2) > 100p(x,C_1)$$또는 $$p(C_2\vert x) > 100p(C_1\vert x)$$를 만족해야 합니다. 쉽게 말해, 이 사람이 건강하다고 나올 확률이 건강하지 않다고 나올 확률보다 100배보다는 커야 한다는 것을 의미합니다.

만약 손실행렬이 주대각선 원소들이 0이고 나머지는 1인 `0-1 Loss` 인 경우, $$\hat{C}(x)$$는 다음과 같이 표현할 수도 있습니다.

$$\hat{C}(x) = argmin_j{1-p(C_j\vert x)} = argmax_jp(C_j\vert x)$$

분류문제에서 확률모델에 의존하는 결정이론은 각 클래스 $$C_k$$에 대해 분포 $$p(x\vert C_k)$$와 사전확률 $$p(C_k)$$을 모두 구한 다음 베이즈 확률을 사용해서 사후확률 $$p(C_k\vert x)$$을 구하게 됩니다. 이는 결합분포로부터 데이터를 샘플링해서 생성하는 방식이므로 **생성모델(Generative model)**이라고 부릅니다.

만약 모든 분포를 다 계산하지 않고 사후확률 $$p(C_k\vert x)$$만 구하는 경우, 이를 **식별모델(Descriminative model)**이라고 합니다.

분류문제에서 확률모델에 의존하지 않는 결정이론은 확률값을 계산하지 않고 오직 **판별함수(Discriminant)**라는 함수에만 의존합니다. 이 판별함수는 주어진 입력 $$x$$를 클래스로 할당하는 함수입니다.

결정이론은 **회귀문제**에서도 사용할 수 있습니다. 이 경우 손실함수를 두 값 차이의 제곱으로 표현할 수 있습니다.

$$L(t, y(x)) = \{y(x) - t\}^2$$

이 경우 손실함수의 기댓값은 다음과 같습니다.

$$E[L] = \int_R\int_X\{y(x)-t\}^2p(x,t)dxdt = \int_X\big(\int_R\{y(x)-t\}^2p(x,t)dt\big)dx = \int_X\big(\int_R\{y(x)-t\}^2p(t|x)dt\big)p(x)dx$$

이 때 $$x$$를 위한 최적의 예측값은 $$y(x) = E_t[t\vert x]$$가 됩니다. 이는 `오일러-라그랑주 방정식`을 통해 구할 수 있습니다.

이 손실함수에서 $$\{y(x) - t\}^2$$를 $$\{y(x) - E[t\vert x] + E[t\vert x] - t\}^2$$로 둔다면 다음과 같이 분해할 수 있게 됩니다.

$$\{y(x) - t\}^2 = \{y(x)-E[t\vert x]\}^2 + 2\{y(x)-E[t\vert x]\}\{E[t\vert x] - t\} + \{E[t\vert x] - t\}^2$$

이 때 가운데 교차항(cross-term) $$2\{y(x)-E[t\vert x]\}\{E[t\vert x] - t\}$$은 소거되어 사라지게 됩니다. 따라서 정리하면 다음과 같게 됩니다.

$$\{y(x) - t\}^2 = \{y(x)-E[t\vert x]\}^2 + \{E[t\vert x] - t\}^2$$

따라서 기댓값을 다음과 같이 분해할 수 있게 됩니다.

$$E[L] = \int\{y(x) - E[t\vert x]\}^2p(x)dx + \int var[t\vert x]p(x)dx$$

첫번째 항 $$\int\{y(x) - E[t\vert x]\}^2p(x)dx$$은 함수 $$y(x)$$에 대한 항으로, $$y(x) = E[t\vert x]$$일 때 기대손실이 최소화되게 됩니다.

두번째 항 $$\int var[t\vert x]p(x)dx$$은 $$x$$에 대해 평균화된 $$t$$의 분산으로, 함수 $$y(x)$$와는 관련이 없는 항입니다. 따라서 이 값은 더이상 줄일 수 없는 기대손실의 최소값이 됩니다.

일반적으로 회귀문제의 경우, 결합분포 $$p(x, t)$$ 또는 조건부확률분포 $$p(t\vert x)$$를 구하는 **추론문제(inference)**를 먼저 해결하게 됩니다. 그 다음 **주변화(marginalize)**를 통해 $$E_t[t\vert x]$$를 구하게 됩니다.
