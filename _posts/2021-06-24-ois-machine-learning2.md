---
layout: post
title: 기계학습과 수학
date:   2021-06-24 17:00:00 +0900
description: 수학적 지식이 기계학습에 어떻게 적용되는지 알아보자
categories: 오일석-기계학습
---

기계학습에서 수학은 목적함수를 정의하거나 목적함수의 최저점을 찾아주는 최적화 이론 등을 제공합니다.

## 선형대수

선형대수(Linear algebra)는 주어진 데이터의 공간을 변환하고 표현하는데 사용됩니다. 이 때 데이터는 특징 벡터로 표현되며, 주로 열벡터로 표현됩니다.

$$x=(x_1,x_2,x_3,x_4)^T$$

이는 2차원 이미지의 경우도 크게 다르지 않습니다. 이미지의 경우 각 픽셀값을 정량화한 후 이를 열벡터로 변환하여 사용합니다.

**행렬(matrix)**은 여러 개의 벡터를 담은 정보를 말합니다. 이 때 행은 데이터를 나타내고 열은 특징을 나타냅니다. 특별히 훈련집합을 담은 행렬을 설계행렬(design matrix)라고 합니다. 행렬은 방정식을 간단하계 표현할 수 있는 장점이 있습니다.

**텐서(tensor)**는 3차원 이상의 구조를 가진 숫자 배열을 말합니다. 위의 벡터는 1차이고, 행렬은 2차 배열입니다. 3차원 구조의 RGB 영상이 대표적인 텐서의 예시입니다.

## 유사도와 놈

두 벡터의 유사도를 구하기 위해선 벡터를 기하학적으로 해석해야 합니다. 이 경우 코사인 유사도를 사용할 수 있으며 식은 다음과 같습니다.

$$cos\_similarity(a,b)=\frac{a}{\vert\vert a\vert\vert}\cdot\frac{a}{\vert\vert b\vert\vert}=cos(\theta)$$

**놈(norm)**은 벡터와 행렬의 크기를 나타냅니다. 벡터의 $$p$$차 놈은 다음과 같이 정의되어 있습니다.

$$\vert\vert x\vert\vert_p=\big(\sum_{i=1,d}\vert x_i\vert^p\big)^{\frac{1}{p}}$$

$$p = \infty$$로 설정한 경우, 이를 최대놈이라고 하며 다음과 같습니다.

$$\vert\vert x\vert\vert_\infty=max(\vert x_1\vert,\vert x_2\vert,...,\vert x_d\vert)$$

이 떄 1차 놈은 $$\vert\vert x\vert\vert_1=\sum_i\vert x_i\vert$$로 정의되어 있으며 이를 맨해튼 거리(Manhattan distance)라고도 합니다. 2차 놈은 $$\vert\vert x\vert\vert_2=\sum_ix_i^2$$으로 정의되어 있으며 이를 유클리드 거리(Euclidean distance)라고도 합니다. 놈은 모델을 규제할 때에도 사용됩니다.

행렬의 경우 프로베니우스 놈(Frobenius norm)을 사용해서 크기를 측정할 수 있습니다.

$$\vert\vert A\vert\vert_F=\big(\sum_{i=1,n}\sum_{j=1,m}a^2_{ij}\big)^{\frac{1}{2}}$$

## 퍼셉트론

**퍼셉트론(perceptron)**이란 사람의 뉴런을 모방한 고전 분류기 모델입니다. 퍼셉트론은 벡터를 입력으로 받아 가중치를 곱하고 활성함수(Activation function) $$\tau$$를 통과시키게 해서 분류하는 모델입니다.

입력 벡터가 $$X=(x_1,x_2,...,x_d)$$로 주어진다면 j번째 퍼셉트론의 가중치 벡터는 $$w_j=(w_{j1},w_{j2},...,w_{jd})^T$$와 같이 표현됩니다. 출력은 $$O = (o_1,o_2,...,o_c)^T$$입니다.

$$o=\tau(w\cdot x)$$

$$\tau(a) = 1(a\geq T),\tau(a)=-1 (a<T)$$

훈련(training)은 $$x, o$$를 아는 상태에서 $$w$$를 설정하는 과정이고, 추론(inferring)은 $$x, w$$를 아는 상태에서 $$o$$를 설정하는 과정입니다.

## 벡터공간

기저벡터(basic vector) a와 b가 있다고 할 때 이 둘을 선형결합하면 새로운 공간을 만들 수 있습니다.

$$c = \alpha_1a+\alpha_2b$$

선형결합으로 만들어진 이 공간을 **벡터공간(vector space)**이라고 합니다.

## 역행렬

정사각행렬 $$A$$의 역행렬 $$A^{-1}$$은 다음을 만족하는 행렬을 말합니다.

$$A^{-1}A=AA^{-1}=I$$

이 떄 역행렬이 존재하기 위해서는 A의 행렬식(determinant)이 반드시 0이 아니어야 합니다. 만약 0인 경우, 역행렬은 존재하지 않습니다. 행렬식은 다음과 같이 정의됩니다.

$$det\begin{pmatrix}a b\\c d\end{pmatrix} = ad-bc$$

$$det\begin{pmatrix}a b c\\d e f\\ g h i\end{pmatrix} = aei + bfg + cdh - ceg - bdi - afh$$

행렬식은 기하학적으로 행렬의 곱에 의한 공간의 변환으로 해석할 수 있습니다. 만약 행렬식이 0이라면, 차원이 축소되어 그 부피를 잃게 됩니다. 만약 행렬식이 1이라면 부피와 방향이 모두 보존됩니다.

반면에 행렬식이 -1이라면 부피만 보존되고 방향은 보존되지 않습니다. 행렬식이 5라면 부피는 5배가 되고 방향은 보존되게 됩니다.

## 행렬 분해

행렬은 고윳값(Eigenvalue) $$\lambda$$와 고유 벡터(Eigenvector) $$v$$를 통해 분해할 수 있습니다. 행렬 A를 다음과 같이 분해합니다.

$$Av = \lambda v$$

이 때 행렬 A는 고유벡터 방향으로 고유값만큼 크기 변환을 시키게 됩니다. 이 고윳값과 고유벡터를 사용해서 행렬 A를 다음과 같이 분해할 수 있습니다.

$$A = Q\Lambda Q^{-1}$$

이를 고유분해(Eigen decomposition)라고 합니다. 이 때 $$Q$$는 고유벡터를 열에 배치한 행렬이고, $$\Lambda$$는 고윳값을 대각선에 배치한 대각행렬입니다.

이 고유분해는 행렬을 분석하기 쉽도록 분해해주기는 하지만 정사각행렬에만 적용가능한 단점이 있습니다.

**특잇값 분해(SVD, Singular value decomposition)**는 행렬이 정사각행렬이든 아니든 상관없이 분해할 수 있도록 하는 방법입니다.

$$A=U\Sigma V^T$$

왼쪽 특이행렬 $$U$$는 $$AA^T$$의 고유벡터를 열에 배치한 $$n*n$$ 크기의 행렬이고, 오른쪽 특이행렬 $$V$$는 $$A^TA$$의 고유벡터를 열에 배치한 $$m*m$$ 행렬입니다. 가운데 $$\Sigma$$는 고윳값의 제곱근을 대각선에 배치한 $$n*m$$ 대각행렬이 됩니다.

특이행렬은 행렬을 회전 변환시키고, 대각행렬은 행렬의 크기를 변환합니다. 즉 특잇값 분해는 행렬을 회전 변환시키고, 크기를 변환하고 다시 회전변환시키게 됩니다.

이러한 특잇값 분해는 역행렬을 쉽게 구하는데 사용됩니다. 예를 들어 정사각행렬이 아닌 행렬도 다음과 같이 역행렬을 구할 수 있습니다.

$$A=U\Sigma V^T$$

$$A^+=V\Sigma'U^T$$

이 때 $$\Sigma'$$는 $$\Sigma$$의 역치를 전치한 행렬입니다.

$$\Sigma=\begin{pmatrix}\sigma_1, 0\\0, \sigma_2\\ 0, 0\end{pmatrix}$$

$$\Sigma'=\begin{pmatrix}\frac{1}{\sigma_1}, 0,0\\0,\frac{1}{\sigma_2},0\end{pmatrix}$$

## 베이즈 정리

베이즈 정리는 사후확률을 우도와 사전확률을 통해 구하는 방식을 말합니다. 확률모델을 사용하는 경우 사후확률을 직접 추정하는 것은 사실상 불가능하므로 많이 사용되는 정리입니다.

$$p(y\vert x)=\frac{p(x\vert y)p(y)}{p(x)}$$

예를 들어 하얀공이 나왔다는 사실만 알고, 어느 병에서 나온지 모를 경우, 다음 식을 통해 이를 추정할 수 있습니다.

$$\hat{y}=argmax_yp(y\vert x)=argmax_y\frac{p(x=하양\vert y)p(y)}{p(x=하양)}$$

이 때 사전확률 $$p(y)$$은 $$p(y=c_i)=\frac{n_i}{n}$$으로 구할 수 있고, 우도는 밀도 추정(Density estimation)으로 구할 수 있습니다.

## 최대우도추정

**최대우도(ML, Maximum likelihood)**는 어떤 확률변수의 관찰된 값들을 토대로 그 확률변수의 분포를 구하는 방법입니다. 확률분포를 모르는 상황에서 이를 추정하기 위해 사용되며, 이 추정을 **최대우도추정(MLE, Maximum likelihood estimation)**이라고 합니다.

$$\hat{\theta}=argmax_\theta p(x\vert\theta)$$

이 때 $$\theta$$는 확률분포이며 $$x$$는 $$\theta$$에 대한 우도입니다. 최대우도추정은 로그 표현으로 바꾸어 곱셈 식을 덧셈 식으로 바꾸어 계산을 단순화하는 최대로그우도추정을 사용하기도 합니다.

$$\hat{\theta}=argmax_\theta logp(x\vert\theta)=argmax_\theta\sum_{i=1}{n}logp(x_i\vert\theta)$$

## 정보이론

**정보이론**이란 사건이 가진 정보를 정량화하는 것에 대한 이론입니다. 이 경우 잘 일어나지 않는 사건이 더 높은 정보량을 가지도록 합니다. 예를 들어 아침에 해가 뜬다는 사건은 정보가 거의 없다고 하며, 아침에 일식이 있었다는 사건은 정보량이 매우 많다고 표현합니다.

사건 하나가 가진 정보를 자기정보(Self information)라고 하며, 보통 로그를 통해 표현합니다. 이 때 밑이 2인 경우를 비트정보, 밑이 자연상수인 경우를 나츠정보라고 합니다.

$$h(e_i)=-log_2p(e_i)$$

$$h(e_i)=-log_ep(e_i)$$

동전에서 앞면이 나오는 사건의 정보량을 $$-log_2\frac{1}{2}=1$$, 주사위에서 1이 나오는 사건의 정보량을 $$-log_2\frac{1}{6}=2.58$$로 표현합니다. 이 경우 주사위에서 1이 나오는 사건의 정보량이 더 큰 것을 알 수 있습니다.

**엔트로피(entropy)**는 모든 사건 정보량의 기대값을 말합니다. 엔트로피는 확률변수의 불확실성을 표현하며, 확률변수가 얼마만큼의 정보를 가질수 있는지를 판단하는데 사용합니다.

$$H(x)=-\sum_{i=1,k}p(e_i)log_2p(e_i)$$

엔트로피는 모든 사건이 동일한 확률을 가질 때, 즉 불확실성이 가장 높은 경우 가장 큰 값을 가지게 됩니다.

## 교차엔트로피

**교차엔트로피(Cross entropy)**는 두 확률분포가 얼마나 정보를 공유하고 있는지를 나타내는 엔트로피를 말합니다. 확률분포 $$P,Q$$에 대해서 교차엔트로피는 다음과 같습니다.

$$H(P,Q)=-\sum_xp(x)log_2Q(x)=-\sum_{i=1,k}p(e_i)log_2Q(e_i)$$

이를 전개하면 다음과 같습니다.

$$H(P,Q)=H(P)+\sum_xp(x)log_2\frac{P(x)}{Q(x)}$$

이 때 $$\sum_xp(x)log_2\frac{P(x)}{Q(x)}$$를 **KL 발산(KL Divergence)**라고 합니다. KL 발산은 두 확률분포 사이의 거리를 나타냅니다. 위의 식에서 $$H(p)$$는 변화하지 않는 값이므로 따라서 교차엔트로피를 최소화하는 것은 KL 발산을 최소화하는 것과 동일한 의미입니다. 교차엔트로피는 딥러닝의 분류 문제 등에서 손실함수로 많이 사용됩니다.

$$P와Q의 교차 엔트로피 H(P,Q)=P의 엔트로피 + P와 Q 간의 KL 발산$$

## 최적화

기계학습은 적절한 모델을 선택하고 목적함수를 정의한 후 모델의 매개변수 공간을 탐색해서 목적함수가 최저가 되는 전략을 사용합니다. 즉 특징 공간이 아닌, 가설로 설정한 모델의 매개변수 공간을 탐색해서 최적점을 찾습니다. 주로 스토캐스틱 경사 하강법(SGD)과 오류 역전파(Backpropagation)를 사용해서 수치적 방법으로 최적의 해를 구하게 됩니다.

$$\hat{\theta}=argmin_\theta J(\theta)$$

이때 모델의 매개변수 공간은 특징 공간에 비해 더 많은 차원을 가집니다. 따라서 전역 최적해를 찾는 것이 아니라 지역 최적해를 찾고 학습을 종료하는 경우가 존재합니다.

이 매개변수는 목적함수가 작아지는 방향을 미분으로 구하여 찾게됩니다. 목적함수의 최저점이 존재하는 방향은 $$-J'(\theta)$$입니다.

```python
난수를 생성하여 초기해 a를 설정한다.
repeat:
    J(a)가 작아지는 방향 d(a)를 구한다.
    a = a + d
until(멈춤조건)
최적해 a = a
```

매개변수 공간에는 여러 매개변수들이 존재하므로, 따라서 편미분을 통해 경사도(gradient)를 구해서 최적의 점을 탐색하게 됩니다. 다만 이 경우 얼마만큼 이동할지에 대해서는 알 수 없습니다. 다층 퍼셉트론의 경우 합성함수를 사용하기 때문에 연쇄법칙을 적용해서 미분하게 됩니다.

야코비안 행렬은 함수를 미분하여 얻은 행렬을 말합니다.

$$J = \begin{pmatrix}\frac{\partial f_1}{\partial x_1} ... \frac{\partial f_1}{\partial x_n} \\ ... ... ... \\ \frac{\partial f_m}{\partial x_1} ... \frac{\partial f_m}{\partial x_n} \end{pmatrix} = \begin{pmatrix}\nabla f_1\\...\\\nabla f_m\end{pmatrix} = \begin{pmatrix}\frac{\partial f}{\partial x_1} ... \frac{\partial f}{\partial x_n}\end{pmatrix}$$

해세 행렬은 2차 편도함수에 대한 행렬을 말합니다.

$$H = \begin{pmatrix}\frac{\partial^2f_1}{\partial x_1x_1} ... \frac{\partial^2f_1}{\partial x_1x_n} \\ ... ... ... \\ \frac{\partial^2f_m}{\partial x_nx_1} ... \frac{\partial^2f_m}{\partial x_nx_n} \end{pmatrix}$$

학습에서 얼마나 이동할지는 학습률(Learning rate)을 통해 조절합니다. 이 방법을 **경사하강법(Gradient descent)**이라고 합니다. 학습의 방향을 $$d\theta=\frac{\partial J}{\partial \theta}$$로 설정한다면, 학습할 이동 거리는 다음과 같이 정의할 수 있습니다.

$$\theta=\theta-pd\theta$$

```python
난수를 생성하여 초기해 a를 설정한다.
repeat:
    X에 있는 샘플의 그레디언트 g1, g2,...,gn을 계산한다.
    g = (g1+g2+...+gn)/n
    a = a - pg
until(멈춤조건)
최적해 a = a
```

이 때 훈련집합을 한번에 다 관찰하고 갱신하는 방식을 배치 방식이라고 하며, 작은 샘플부터 경사도를 계산하면서 즉시 갱신하는 방식을 확률론적, 또는 스토캐스틱(Stochastic) 방식이라고 합니다. 배치 방식은 정확하게 수렴하지만 느리다는 단점이 있고, 스토캐스틱 방식은 수렴이 헤멜수도 있으나 빠르다는 장점이 있습니다.

```python
난수를 생성하여 초기해 a를 설정한다.
repeat:
    X의 샘플의 순서를 섞는다.
    for (j = 1 to n):
        j번째 그레디언트 gj를 계산한다.
        a = a - pg
until(멈춤조건)
최적해 a = a
```

현실에서는 스토캐스틱 경사 하강법에 여러 제어 알고리즘을 사용하는 방법을 많이 사용한다.
