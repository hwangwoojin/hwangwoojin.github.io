---
layout: post
title: 선형회귀
date:   2021-06-22 19:00:00 +0900
description: 선형회귀에 대해서 알아보자
categories: 프로그래머스-인공지능-데브코스
---

**선형회귀(Linear Regression))**란 선형함수를 사용해서 모델을 예측하는 방법을 말합니다.

$$y(x,w)=w_0+w_1x_1+...+w_Dx_D$$

만약 비선형인 함수를 만들고 싶다면 기저함수(basic function)를 사용하면 된다는 것을 이미 다룬바 있습니다.

$$y(x,w) = w_0+\sum_{j=1}^{M-1}w_j\phi_j(x)$$

기저함수 $$\phi(x)$$는 $$x^j$$ 형태의 다항식이 될 수도 있고, 가우시안이 될 수도 있으며, 혹은 시그모이드 함수가 될 수도 있습니다.

다음과 같은 확률변수를 생각해보겠습니다.

$$t = y(x,w) + \epsilon$$

여기서 $$y(x, w)$$는 결정론적인 함수이며, $$\epsilon$$은 노이즈 값입니다. 만약 노이즈가 가우시안 분포 $$N(\epsilon\vert 0,\beta^-1))을 따르는 확률변수라면 $$t$$의 분포는 이렇게 쓸수도 있습니다.

$$p(t\vert x,w,\beta) = N(t\vert y(x,w),\beta^{-1})$$

이 함수에서 제곱합을 통해 에러함수를 표현하고자 한다면 다음과 같이 사용할 수 있습니다.

$$E_D(w) = \frac{1}{2}\sum_{n=1}^N(t_n-w^T\phi(x_n),\beta^{-1})$$

최대우도추정법(MLE)을 사용하여 확률변수의 파라미터인 $$x,w$$를 찾을 수도 있는데, 이 때 우도함수를 최대화시키는 $$w$$ 값은 식을 풀어보면 위의 제곱합 에러함수를 최소화시키는 값과 동일하게 됩니다.

$$w$$에 대한 기울기(gradient) 벡터는 다음과 같이 표현할 수 있습니다.

$$\nabla \ln p(t\vert w,\beta)=\sum_{n=1}^N\{t_n-w^T\phi(x_n)\}\phi(x_n)^T)$$

이 때 최적값은 $$W_{ML}=(\phi^T\phi)^{-1}\phi^Tt$$로 구할 수 있습니다. 이 식을 **정규방정식(normal equations)**이라고 합니다.
