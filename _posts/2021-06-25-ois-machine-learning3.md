---
layout: post
title: (오일석 기계학습 정리 3) 다층 퍼셉트론
date:   2021-06-25 00:00:00 +0900
description: 타층 퍼셉트론에 대해 알아보자
categories: 프로그래머스-인공지능-데브코스
---

## 퍼셉트론

신경망(Neural network)은 사람의 뉴런을 모방한 네트워크입니다. 뉴런은 사람의 뇌에 존재하는 가장 작은 정보처리 단위입니다. 뇌는 뉴런과 뉴런이 연결된 네트워크이며 이를 모방한 것이 신경망입니다. 사람의 신경망이 세포체, 수상돌기, 축삭, 시냅스 등으로 구성되어 있다면, 인공 신경망은 노드, 입력, 출력, 가중치 등으로 구성되어 있습니다.

**퍼셉트론(Perceptron)**은 노드, 가중치, 층으로 구성된 구조의 신경망입니다. 퍼셉트론은 원시적 신경망이지만, 현대 인공 신경망의 토대가 된 구조입니다.

퍼셉트론은 모델의 매개변수와 조건에 의해 출력이 완전히 결정되는 결정론적(Deterministic) 형태와 매개변수와 조건이 같더라도 그 결과가 달라질 수 있는 확률론적 또는 스토캐스틱(stochastic) 형태가 존재합니다.

스토캐스틱 형태의 퍼셉트론 알고리즘은 다음과 같습니다.

```python
난수를 생성하여 초기해 w를 설정한다.
repeat:
    X의 샘플 순서를 섞는다.
    quit = True
    for j=1 to n:
        y = t(w * xj)
        if y != yj:
            quit = False
            for j=0 to d:
                wi = wi + pyjxji
until(quit)
최종 w = w
```

퍼셉트론에는 입력 벡터 $$X=(x_1,x_2,...,x_d)^T$$, 출력 값 -1 또는 1, 그리고 그 사이를 연결하는 단일 노드 층이 존재합니다. 퍼셉트론의 입력값에는 항상 1이 입력되는 편향(bias) 노드가 존재합니다.

$$y=\tau(w_0+\sum_{i=1}^dw_ix_i)$$

입력값과 가중치를 곱하고 모두 더해 s를 구합니다. 그리고 활성함수를 적용합니다. 활성함수 $$\tau$$로는 계단함수를 사용합니다. 퍼셉트론을 행렬로 표현하면 다음과 같습니다.

$$s = w^Tx + w_0, x = (x_1,x_2,...,x_d)^T, w=(w_1,w_2,...,w_d)^T$$

$$s=w^Tx, x=(1,x_1,x_2,...,x_d)^T, w=(w_0,w_1,...,w_d)^T$$

$$y=\tau(w^Tx)$$

퍼셉트론은 기하학적으로 특징 공간을 1과 -1로 이분할하는 결정직선의 의미를 가집니다. 여기서 가중치는 기울기가 되고 편향은 절편이 됩니다.

퍼셉트론의 학습은 실제값과 예측값이 일치하는 방향으로 가중치를 개선하는 작업입니다. 일반적인 퍼셉트론의 경우, 먼저 과업과 목적함수를 정의하고 이후 목적함수를 최소화하는 최적화 과정을 수행하게 됩니다.

## 퍼셉트론의 목적함수

퍼셉트론의 목적함수 또는 손실함수 $$J(w)$$는 샘플을 잘 맞힐수록 작은 값을 가지고, 샘플을 많이 틀릴수록 큰 값을 가지는 함수입니다. 또 목적함수는 항상 양수이어야 한다는 특징이 있습니다. 틀린 샘플의 집합 $$Y$$에 대해 다음 식을 생각해보겠습니다.

$$J(w)=\sum_{x_k\in Y}-y_k(w^Tx_k)$$

이 식은 항상 양수값을 가지고, 틀린 샘플이 많을 수록 큰 값을 가지며, 만약 모든 샘플을 맞추면 값이 0이 되므로 목적함수로 사용할 수 있습니다.

목적함수를 정의했다면, 경사 하강법(Gradient descent)을 사용하여 최솟값을 찾게 됩니다. 목적함수를 미분하여 기울기를 구하고, 감소하는 방향으로 탐색하여 극값을 찾습니다.

$$\frac{\partial J(w)}{\partial w_i} = \sum_{x_k\in Y}\frac{\partial(-y_k(w_0x_{k0}+w_1x_{k1}+...+w_dx_{kd}))}{\partial w_i}=\sum_{x_k\in Y}-y_kx_{ki}$$

이 때 $$x_{ki}$$는 $$x_k=(x_{k0},x_{k1},...,x_{kd})^T$$의 $$i$$번째 요소를 말합니다. 학습률 $$p$$를 통해 학습한다고 하면 가중치는 $$\theta=\theta-pg$$로 개선합니다. 가중치 하나를 개선한다고 하면 다음과 같은 식을 사용합니다.

$$w_i=w_i+p\sum_{x_k\in Y}y_kx_{ki}$$

이를 **델타규칙(Delta rule)**이라고 합니다. 델타규칙은 퍼셉트론의 기본적인 학습 방법입니다. 델타규칙을 행렬 표기로 수정하면 다음과 같습니다.

$$w = w + py_jx_j$$
