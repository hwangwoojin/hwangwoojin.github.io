---
layout: post
title: 다층 퍼셉트론
date:   2021-06-25 00:00:00 +0900
description: 타층 퍼셉트론에 대해 알아보자
categories: 오일석-기계학습
---

> 오일석 교수님의 책 [기계 학습](http://www.yes24.com/Product/Goods/57537091)을 참고하였습니다.

## 퍼셉트론

신경망(Neural network)은 사람의 뉴런을 모방한 네트워크입니다. 뉴런은 사람의 뇌에 존재하는 가장 작은 정보처리 단위입니다. 뇌는 뉴런과 뉴런이 연결된 네트워크이며 이를 모방한 것이 신경망입니다. 사람의 신경망이 세포체, 수상돌기, 축삭, 시냅스 등으로 구성되어 있다면, 인공 신경망은 노드, 입력, 출력, 가중치 등으로 구성되어 있습니다.

**퍼셉트론(Perceptron)**은 노드, 가중치, 층으로 구성된 구조의 신경망입니다. 퍼셉트론은 원시적 신경망이지만, 현대 인공 신경망의 토대가 된 구조입니다.

퍼셉트론은 모델의 매개변수와 조건에 의해 출력이 완전히 결정되는 결정론적(Deterministic) 형태와 매개변수와 조건이 같더라도 그 결과가 달라질 수 있는 확률론적 또는 스토캐스틱(stochastic) 형태가 존재합니다.

스토캐스틱 형태의 퍼셉트론 알고리즘은 다음과 같습니다.

```python
난수를 생성하여 초기해 w를 설정한다.
repeat:
    X의 샘플 순서를 섞는다.
    quit = True
    for j=1 to n:
        y = t(w * xj)
        if y != yj:
            quit = False
            for j=0 to d:
                wi = wi + pyjxji
until(quit)
최종 w = w
```

퍼셉트론에는 입력 벡터 $$X=(x_1,x_2,...,x_d)^T$$, 출력 값 -1 또는 1, 그리고 그 사이를 연결하는 단일 노드 층이 존재합니다. 퍼셉트론의 입력값에는 항상 1이 입력되는 편향(bias) 노드가 존재합니다.

$$y=\tau(w_0+\sum_{i=1}^dw_ix_i)$$

입력값과 가중치를 곱하고 모두 더해 s를 구합니다. 그리고 활성함수를 적용합니다. 활성함수 $$\tau$$로는 계단함수를 사용합니다. 퍼셉트론을 행렬로 표현하면 다음과 같습니다.

$$s = w^Tx + w_0, x = (x_1,x_2,...,x_d)^T, w=(w_1,w_2,...,w_d)^T$$

$$s=w^Tx, x=(1,x_1,x_2,...,x_d)^T, w=(w_0,w_1,...,w_d)^T$$

$$y=\tau(w^Tx)$$

퍼셉트론은 기하학적으로 특징 공간을 1과 -1로 이분할하는 결정직선의 의미를 가집니다. 여기서 가중치는 기울기가 되고 편향은 절편이 됩니다.

퍼셉트론의 학습은 실제값과 예측값이 일치하는 방향으로 가중치를 개선하는 작업입니다. 일반적인 퍼셉트론의 경우, 먼저 과업과 목적함수를 정의하고 이후 목적함수를 최소화하는 최적화 과정을 수행하게 됩니다.

## 퍼셉트론의 목적함수

퍼셉트론의 목적함수 또는 손실함수 $$J(w)$$는 샘플을 잘 맞힐수록 작은 값을 가지고, 샘플을 많이 틀릴수록 큰 값을 가지는 함수입니다. 또 목적함수는 항상 양수이어야 한다는 특징이 있습니다. 틀린 샘플의 집합 $$Y$$에 대해 다음 식을 생각해보겠습니다.

$$J(w)=\sum_{x_k\in Y}-y_k(w^Tx_k)$$

이 식은 항상 양수값을 가지고, 틀린 샘플이 많을 수록 큰 값을 가지며, 만약 모든 샘플을 맞추면 값이 0이 되므로 목적함수로 사용할 수 있습니다.

목적함수를 정의했다면, 경사 하강법(Gradient descent)을 사용하여 최솟값을 찾게 됩니다. 목적함수를 미분하여 기울기를 구하고, 감소하는 방향으로 탐색하여 극값을 찾습니다.

$$\frac{\partial J(w)}{\partial w_i} = \sum_{x_k\in Y}\frac{\partial(-y_k(w_0x_{k0}+w_1x_{k1}+...+w_dx_{kd}))}{\partial w_i}=\sum_{x_k\in Y}-y_kx_{ki}$$

이 때 $$x_{ki}$$는 $$x_k=(x_{k0},x_{k1},...,x_{kd})^T$$의 $$i$$번째 요소를 말합니다. 학습률 $$p$$를 통해 학습한다고 하면 가중치는 $$\theta=\theta-pg$$로 개선합니다. 가중치 하나를 개선한다고 하면 다음과 같은 식을 사용합니다.

$$w_i=w_i+p\sum_{x_k\in Y}y_kx_{ki}$$

이를 **델타규칙(Delta rule)**이라고 합니다. 델타규칙은 퍼셉트론의 기본적인 학습 방법입니다. 델타규칙을 행렬 표기로 수정하면 다음과 같습니다.

$$w = w + py_jx_j$$

## 다층 퍼셉트론

퍼셉트론은 선형 분류기이므로 선형 분리 불가능한 상황에서 오류를 보이게 됩니다. 예를 들어 XOR 문제에서 퍼셉트론은 75%의 정확도 한계를 가지게 됩니다.

**다층 퍼셉트론(Multi-layer perceptron)**은 여러 개의 퍼셉트론을 사용하여 선형 분류기의 한계를 극복하는 방식입니다. 입력 벡터를 여러 퍼셉트론을 통해 처리한 후, 이 퍼셉트론에서 구해진 결과 벡터들을 다시 가중치를 곱해서 결과값을 구하는 방식입니다. 이 경우 선형 분리 불가능한 문제도 공간 변환을 통해 선형 분리 가능하도록 만들 수 있습니다.

다층 퍼셉트론의 특징은 크게 3가지입니다. 먼저, 다층 퍼셉트론은 특징 공간을 분류하기에 편한 공간으로 변환하는 여러 **은닉층(Hidden layer)**을 가집니다. 분류에 더 유리한 특징 공간으로 변환하는 이러한 작업을 특징학습(Feature learning)이라고 하며, 이러한 관점에서 은닉층을 특징 추출기라고 부르기도 합니다. 은닉층의 노드 수가 지나치게 많으면 과적합 현상이 발생하고, 너무 작다면 과소적합 문제가 발생합니다.

다음으로, 다층 퍼셉트론은 기존 퍼셉트론과는 달리 **시그모이드 함수**를 활성함수로 사용합니다. 계단 함수는 -1과 1의 양극화된 결과를 반환했지만, 시그모이드 함수는 이를 좀 더 부드럽고 연속적으로 처리할 수 있도록 합니다.

$$sigmoid(x)=\frac{1}{1+e^{-ax}}$$

마지막으로, **오류 역전파 알고리즘(Error backpropagation)**을 사용합니다. 다층 퍼셉트론은 여러 층이 순차적으로 이어진 구조이므로, 역방향으로 진행하면서 한 번에 한 층씩 그레디언트를 계산하고 가중치를 갱신하게 됩니다.

퍼셉트론의 은닉층에서는 로지스틱 시그모이드(Logistic sigmoid) 함수를 주로 사용하는데, 이 경우 함수가 넓은 포화곡선 형태를 가지게 되므로 그레디언트 손실 문제가 발생하여 오류 역전파를 구하기 어려운 문제가 존재합니다. 따라서 이후 설명할 깊은 신경망에서는 주로 ReLU(Rectifier linear unit)를 사용합니다.

퍼셉트론은 특징벡터 x를 출력 벡터 o로 매핑하는 함수입니다. 퍼셉트론이 2개의 층을 가지는 경우는 $$o=f_2(f_1(x))$$, 3개의 층을 가지는 경우는 $$o=f_3(f_2(f_1(x)))$$와 같이 표현할 수 있습니다. 층이 4개 이상일 경우, 이를 일반적으로 깊은 신경망이라고 부릅니다.

퍼셉트론의 가중치는 입력층과 은닉층을 연결하는 가중치, 은닉층의 가중치, 은닉층과 출력층을 연결하는 가중치로 구성됩니다. 이 때 입력을 0번째 은닉층, 출력을 마지막 은닉층이라고 간주한다면 가중치 행렬 $$U$$의 요소 $$u_{ji}^l$$을 l-1번째 은닉층의 i번째 노드를 l번째 은닉층의 j번째 노드와 연결하는 가중치로 정의할 수 있습니다.

$$z_j=\tau(u_j^1x)$$

$$o_k=\tau(u_k^2z)$$

이를 한번에 행렬로 표현하면 다음과 같습니다.

$$o=\tau(U^2\tau_h(U^1x))$$

이를 알고리즘으로 구현하면 다음과 같습니다.

```python
테스트 샘플 x에 바이어스를 추가한다.
zsum = U[1]x
z = t(zsum)
osum = U[2]z
o = t(osum)
o에서 가장 큰 값을 가지는 노드에 해당하는 클래스를 y에 대입한다.
```

가장 마지막 줄을 식으로 표현하면 다음과 같습니다.

$$y=argmax_ko_K$$

이는 입력벡터 x가 주어질 때, 여기에 입력층 가중치를 내적하고 활성함수를 통과시킨 값을 출력층 가중치와 내적하고 활성함수를 통과시킨 것을 의미합니다. 이 식은 은닉층이 하나인 경우에 해당하지만, 만약 은닉층이 여러개인 경우, 이 식은 $$U_3, U_4...$$ 등의 여러 은닉층 가중치를 사용하게 됩니다.

## 오류 역전파 알고리즘

오류 역전파 알고리즘이란 순방향으로 학습을 할 때 특징공간을 변환시키면서 가중치를 전파하고 계산한 오차를 역방향으로 전파하면서 가중치를 개선하는 방식을 말합니다.

학습을 진행할 때 가중치는 출력값에 영향을 미치게 됩니다. 이 떄 어느정도 영향을 주는지, 얼마나 변화시켜야 출력값이 변하는지를 알아야 가중치를 조절할 수 있습니다. 즉 손실된 정도에 원하는 가중치를 미분한 값을 구해야 합니다.

$$\frac{\partial E}{\partial w_{ij}}$$

이는 미분의 연쇄법칙을 통해서 구할 수 있습니다.

$$\frac{\partial E}{\partial w_{ij}}=\frac{\partial E}{\partial o_j}\frac{\partial o_j}{\partial net_j}\frac{\partial net_j}{\partial w_ij}$$

이 떄 $$\frac{\partial E}{\partial o_j}$$는 얼마나 결과벡터를 변경해야 에러가 변하는지에 대한 미분이고, $$\frac{\partial o_j}{\partial net_j}$$는 얼마나 신경망의 층을 변경해야 출력벡터가 변하는지에 대한 미분이며, $$\frac{\partial net_j}{\partial w_ij}$$는 얼마나 가중치를 변경해야 해당 신경망의 층이 변하는지에 대한 미분입니다.

```python
U[1]과 U[2]를 초기화한다.
repeat:
    X의 순서를 섞는다.
    for (X 샘플 각각에 대해):
        전방 계산을 하여 o를 구한다.
        g[1]과 g[2]를 계산한다.
        U = U - pg를 통해 U[1], U[2]를 갱신한다.
until(멈춤 조건)
```
