---
layout: post
title: Torch
date:   2021-06-25 03:00:00 +0900
description: Torch 의 기본적인 사용법에 대해 알아보자
categories: 프로그래머스-인공지능-데브코스
---

torch, 또는 pytorch는 numpy를 대체하면서 GPU를 이용하여 연산을 수행하여야 할 때 주로 사용되는 파이썬 패키지입니다. torch는 유연하고 빠른 속도를 제공하며 사용하기 편리하다는 장점이 있습니다.

torch에 대한 더 많은 정보는 [pytorch 공식문서](https://pytorch.org/docs/torch)에서 확인할 수 있습니다.

## torch 모듈 가져오기

코드는 구글 코랩 환경에서 수행하였습니다.

```python
import torch

torch.__version
# '1.9.0+cu102'
```

## 초기화

`torch.empty`를 사용하면 dummy 값이 들어있는 행렬을 생성할 수 있습니다.

```python
x = torch.empty(5,3)
x
# tensor([[-1.2911e-25,  3.0817e-41,  1.4714e-43],
#         [ 1.5414e-43,  1.6255e-43,  1.3312e-43],
#         [ 1.4293e-43,  1.6395e-43,  1.5414e-43],
#         [ 1.3873e-43,  1.6255e-43,  1.4714e-43],
#         [ 1.5554e-43,  1.5414e-43,  0.0000e+00]])
```

dummy 값이 아니라 무작위 수로 초기화를 시키고 싶을때는 `torch.rand`를 사용합니다. 이 경우 값은 0에서 1사이의 무작위 값이 됩니다.

```python
x = torch.rand(5,3)
x
# tensor([[0.5770, 0.6083, 0.4877],
#         [0.6824, 0.2345, 0.5173],
#         [0.6957, 0.2397, 0.6933],
#         [0.4166, 0.5997, 0.6058],
#         [0.9620, 0.1481, 0.2928]])
```

정규화된 무작위 값으로 초기화하고 싶다면 `torch.randn`을 사용하면 됩니다.

```python
x = torch.randn(5,3)
x
# tensor([[ 0.7567,  0.3626, -1.2892],
#         [ 0.6176,  0.0389,  0.7162],
#         [-1.1769,  0.7687,  1.1605],
#         [ 0.1072, -0.5808, -0.0209],
#         [ 0.6547, -1.9007, -0.1653]])
```

초기화할 때 `dtype` 값을 설정하게 되면 데이터의 타입을 설정해줄 수 있습니다. 아래 코드는 `torch.long`을 사용하여 데이터의 타입을 `long` 타입으로 초기화합니다.

```python
x = torch.zeros(5,3, dtype=torch.long)
x
# tensor([[0, 0, 0],
#         [0, 0, 0],
#         [0, 0, 0],
#         [0, 0, 0],
#         [0, 0, 0]])
```

같은 크기의 행렬을 생성하고 싶을때는 `torch.randn_like`을 사용합니다. 예를 들어 크기가 5x3인 행렬이 존재할 때, 크기만 5x3으로 똑같이 설정하고 값은 무작위로 선택하게 됩니다.

```python
x = torch.zeros(5,3)
x = torch.randn_like(x, dtype=torch.float)
x
# tensor([[ 0.0664, -0.9854,  0.6125],
#         [ 0.3649,  2.4456, -0.4290],
#         [-0.0961, -0.8612,  0.0302],
#         [-0.1682, -0.2838, -2.0464],
#         [-2.1773, -0.1728, -0.0478]])
```

또는 `torch.tensor`를 사용하면 데이터를 통해 직접 텐서를 생성할 수 있습니다.

```python
x = torch.tensor([5.5, 3])
x
# tensor([5.5000, 3.0000])
```

## 크기 구하기

행렬의 크기는 `size()` 함수 또는 `shape`를 사용해서 구할 수 있습니다. 둘 다 행렬의 크기를 반환합니다.

```python
x = torch.zeros(5,3)
x.size()
# torch.Size([5, 3])
```

```python
x.shape
# torch.Size([5, 3])
```

## 기본 연산

다음과 같은 행렬을 생성하도록 하겠습니다.

```python
x = torch.rand(5,3)
x
# tensor([[0.3506, 0.3039, 0.4592],
#        [0.8768, 0.9219, 0.3207],
#         [0.3119, 0.5506, 0.3715],
#         [0.5769, 0.5330, 0.1472],
#         [0.6825, 0.4978, 0.0182]])
```

```python
y = torch.rand(5,3)
y
# tensor([[0.3556, 0.7229, 0.0426],
#         [0.9642, 0.2287, 0.0678],
#         [0.9469, 0.7606, 0.5402],
#         [0.8747, 0.7880, 0.9747],
#         [0.3918, 0.4860, 0.8520]])
```

덧셈을 할 떄는 더하기 기호를 사용하거나 `torch.add`를 사용하여 행렬의 덧셈을 수행할 수 있습니다.

```python
x+y
# tensor([[0.7062, 1.0268, 0.5018],
#         [1.8410, 1.1506, 0.3886],
#         [1.2588, 1.3112, 0.9118],
#         [1.4515, 1.3210, 1.1219],
#         [1.0743, 0.9838, 0.8702]])
```

```python
torch.add(x,y)
# tensor([[0.7062, 1.0268, 0.5018],
#         [1.8410, 1.1506, 0.3886],
#         [1.2588, 1.3112, 0.9118],
#         [1.4515, 1.3210, 1.1219],
#         [1.0743, 0.9838, 0.8702]])
```

`torch.add` 함수에서 `out` 파라미터에 행렬을 주게 된다면, 더한 결과값을 `out` 행렬에 대입하게 됩니다.
```python
result = torch.empty(5,3)
torch.add(x,y, out=result)
result
# tensor([[0.7062, 1.0268, 0.5018],
#         [1.8410, 1.1506, 0.3886],
#         [1.2588, 1.3112, 0.9118],
#         [1.4515, 1.3210, 1.1219],
#         [1.0743, 0.9838, 0.8702]])
```

또는 `add_`를 사용하면 inplace 연산을 수행하여 덧셈을 수행할 수 있습니다. 참고로 torch에서 마지막에 `_`가 포함된 함수는 대부분 inplace에 대한 연산을 의미합니다.

```python
y.add_(x)
y
# tensor([[0.7062, 1.0268, 0.5018],
#         [1.8410, 1.1506, 0.3886],
#         [1.2588, 1.3112, 0.9118],
#         [1.4515, 1.3210, 1.1219],
#         [1.0743, 0.9838, 0.8702]])
```

## 원소 참조

torch는 기본적으로 파이썬 리스트와 유사한 참조 방식을 제공하지만, torch만의 유용한 참조 방식을 제공합니다.

```python
x[:, 1]
```

이 코드는 x 행렬의 모든 행, 1번째 열을 참조하라는 것을 의미합니다. 즉 다음과 같습니다.

```python
x = torch.rand(5,3)
x
# tensor([[0.3506, 0.3039, 0.4592],
#         [0.8768, 0.9219, 0.3207],
#         [0.3119, 0.5506, 0.3715],
#         [0.5769, 0.5330, 0.1472],
#         [0.6825, 0.4978, 0.0182]])
```

만약 행렬에 단 하나의 요소만 존재한다고 한다면 `torch.item`을 사용해서 그 값을 그대로 꺼낼수도 있습니다.

```python
x = torch.randn(1)
x
# tensor([0.0526])
```

```python
x.item()
0.052565135061740875
```

```python
x[:, 1]
# tensor([0.3039, 0.9219, 0.5506, 0.5330, 0.4978])
```

## 차원 변경

torch에서는 `view`를 통해 행렬의 차원을 변경할 수 있습니다. 예를 들어 4x4 크기의 다음 행렬이 존재한다고 하겠습니다.

```python
x = torch.randn(4,4)
x.shape
# torch.Size([4,4])
```

만약 이 행렬을 16x1, 즉 차원을 하나 축소시킨 행렬로 표현하고 싶다면, 다음과 같이 구현하면 됩니다.

```python
y = x.view(16)
y.shape
# torch.Size([16])
```

-1 값을 넣을 경우, 자동으로 남은 값을 구해서 넣도록 합니다. 예를 들어 16x1 크기의 행렬을 -1x2로 변환한다면, 이는 자동으로 8x2로 변환됩니다.

```python
z = x.view(-1,2)
z.shpae([8,2])
```

## numpy 변환

torch 행렬은 `numpy`를 사용해서 쉽게 numpy 행렬로 변환할 수 있습니다.

```python
a = torch.ones(5)
a
# tensor([1., 1., 1., 1., 1.])
```

```python
b = a.numpy()
b
# array([1., 1., 1., 1., 1.], dtype=float32)
```

이 떄 주의해야 할 점은 `numpy` 함수로 변환된 numpy 행렬과 기존 torch 행렬이 같은 메모리를 공유한다는 것입니다. 즉 a 행렬에서 연산이 발생해서 값이 변경된다면, b 행렬에서도 바뀐 값이 적용되게 됩니다.

역으로 numpy 행렬을 torch 행렬로 변환할 수도 있습니다. 이 경우 `from_numpy`를 사용합니다.

```python
import numpy as np
a = np.ones(5)
a
# array([1., 1., 1., 1., 1.])
```

```python
b = torch.from_numpy(a)
b
# tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
```

## GPU 사용

torch 행렬에서 GPU를 사용하는 방법은 크게 3가지가 존재합니다. 하나는 `device` 파라미터에 직접 `cuda:0`을 입력하여 GPU 상에 행렬을 생성하는 방법입니다.

```python
x = torch.rand(4,4)
if torch.cuda.is_available():
    device = "cuda:0"
    y = torch.ones_like(x, device=device)
    ...
```

두번째는, `to` 함수를 사용하는 방법입니다.

```python
x = torch.rand(4,4)
if torch.cuda.is_available():
    device = "cuda:0"
    x = x.to(device)
    ...
```

마지막은 `cuda` 함수를 사용하는 방법입니다.

```python
x = torch.rand(4,4)
if torch.cuda.is_available():
    x = x.cuda()
```

## 미분

`torch` 텐서를 생성할 때 `required_grad` 파라미터를 `True`로 설정하게되면 해당 텐서의 연산 과정을 추적할 수 있습니다. 텐서에서 `grad_fn`을 통해 이전 연산을 가져올 수 있으며, 만약 아래와 같이 연산이 존재하지 않는 경우 `None`을 반환하게 됩니다.

```python
x = torch.ones(2, 2, requires_grad=True)
x.grad_fn
# None
```

또는 다음과 같이 `requires_grad` 파라미터를 설정할 수 있습니다.

```python
a = torch.randn(2, 2)
a.requires_grad
# False

a.requires_grad_(True)
a.requires_grad
# True
```

`with torch.no_grad()`를 사용해서 조절하기도 합니다. 이 경우 `requires_grad=True`인 텐서를 `False`로 하여 연산을 추적하는 것을 멈추게 됩니다.

```python
print(x.requires_grad)
print((x ** 2).requires_grad)

with torch.no_grad():
    print((x ** 2).requires_grad)
```

`detach`를 사용해서 내용은 같지만 `requires_grad`만 다른 텐서를 가져오기도 합니다.

```python
print(x.requires_grad)
# True

y = x.detach()
print(y.requires_grad)
# False
```

만약 덧셈 연산을 수행한다면 다음과 같이 `AddBackward`가 생성됩니다.

```python
y = x + 2
y.grad_fn
# <AddBackward0 object at ...>
```

덧셈연산 말고도 곱셈이나, 평군 연산등에도 적용할 수 있습니다.

```python
z = y * y * 3
z.grad_fn
# <MulBackward0 object at ...>

out = z.mean()
out.grad_fn
# <MeanBackward0> object at ...
```

전체 연산에서 중간 값에 대한 미분 값을 유지하고 싶다면 `retain_grad`를 사용하면 됩니다. 이 경우 `retain_graph` 파라미터를 `True`로 설정해야 여러번 미분을 진행할 수 있습니다. 이를 사용해서 다음과 같이 연산의 그레디언트를 구할 수 있습니다.

```python
x = torch.ones(2, 2, requires_grad=True)
y = x + 2
z = y * y * 3
out = z.mean()

print(out)
# tensor(27., grad_fn=<MeanBackward0>)

y.retain_grad()
out.backward(retain_graph=True)

print(x.grad)
# tensor([[4.5000, 4.5000],
#         [4.5000, 4.5000]])

print(y.grad)
# tensor([[4.5000, 4.5000],
#         [4.5000, 4.5000]])

print(z.grad)
# None

out.backward()
print(x.grad)
# tensor([[9., 9.],
#         [9., 9.]])

print(y.grad)
# tensor([[9., 9.],
#         [9., 9.]])
```

## 신경망, 손실함수, 가중치 갱신

`torch.nn` 패키지를 사용하면 인공신경망을 만들 수 있습니다. 이때 `nn`은 모델을 정의하게 되고, 위에서 사용한 `autograd`는 미분에 사용됩니다. `nn.module`은 기본적으로 **계층(layer)**과 **forward 메서드**를 포함해야 합니다. 계층은 모델의 층을 정의하고, forward 메서드는 입력을 받아 출력을 반환합니다.

```python
import torch
import torch.nn as nn

class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()

        self.layer0 = nn.Linear(4, 128)
        self.layer1 = nn.Linear(128, 64)
        self.layer2 = nn.Linear(64, 32)
        self.layer3 = nn.Linear(32, 16)
        self.layer4 = nn.Linear(16, 3)

        self.bn0 = nn.BatchNorm1d(128)
        self.bn1 = nn.BatchNorm1d(64)
        self.bn2 = nn.BatchNorm1d(32)

        self.act = nn.ReLU()

    def forward(self, x):
      x = self.act(self.bn0(self.layer0(x)))
      x = self.act(self.bn1(self.layer1(x)))
      x = self.act(self.bn2(self.layer2(x)))
      x = self.act(self.layer3(x))
      x = self.layer4(x)

      return x
```

신경망에서 forward 메서드만 정의하면 backward 메서드는 자동으로 정의됩니다. 또 모델의 파라미터를 알고싶다면 `net.parameters()` 메서드를 사용하면 됩니다.

신경망을 만들었다면, 다음은 손실 함수를 정의해야 합니다. **손실 함수(Loss function)**란 출력과 라벨값을 받아 출력이 정답으로부터 얼마나 떨어져있는지를 정하는 함수입니다. 여기서는 크로스엔트로피 함수를 손실함수로 정의하도록 하겠습니다.

```python
criterion = nn.CrossEntropyLoss()
```

마지막으로, 가중치를 갱신하는 규칙을 설정해야 합니다. **가중치 갱신**은 옵티마이저(optimizer)를 통해서 수행되며, 다음과 같은 식으로 표현됩니다.

$$가중치(weight)=가중치(weight)-학습률(learning_rate)*변화도(gradient)$$

`torch`에서는 `torch.optim` 패키지에 여러 옵티마이저가 정의되어 있습니다. 여기서는 스토캐스틱 경사 하강법(SGD)를 사용하겠습니다.

```python
import torch.optim as optim

optimizer = optim.SGD(net.parameters(), lr=0.001)
```

## iris 데이터 학습하기

`iris` 데이터는 붓꽃에 대한 간단한 이미지 데이터입니다. 위에서 설정한 신경망, 손실함수, 옵티마이저를 사용하여 학습을 수행해보도록 하겠습니다.

```python
from sklearn.datasets import load_iris
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split

dataset = load_iris()

data, label = dataset.data, dataset.target
# 데이터셋에 대한 정보를 확인하고 싶다면,
# print(dataset.DESCR)

# 데이터셋을 훈련과 테스트로 분리하기
X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.25)

# DataLoader 생성하기
X_train = torch.from_numpy(X_train).float()
y_train = torch.from_numpy(y_train).long()

X_test = torch.from_numpy(X_test).float()
y_test = torch.from_numpy(y_test).long()

train_set = TensorDataset(X_train, y_train)

train_loader = DataLoader(train_set, batch_size=4, shuffle=True)

# 신경망, 손실함수, 옵티마이저 정의하기
net = Net()
optimizer = torch.optim.SGD(net.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()
epochs = 200

losses = list()
accuracies = list()

# 훈련데이터 학습하기
for epoch in range(epochs):
  epoch_loss = 0  
  epoch_accuracy = 0
  for X, y in train_loader:
    optimizer.zero_grad()
    output = net(X)

    loss = criterion(output, y)
    loss.backward()
    
    optimizer.step()

    _, predicted = torch.max(output, dim=1)
    accuracy = (predicted == y).sum().item()
    epoch_loss += loss.item()
    epoch_accuracy += accuracy
  

  epoch_loss /= len(train_loader)
  epoch_accuracy /= len(X_train)
  print("epoch :{}, \tloss :{}, \taccuracy :{}".format(str(epoch+1).zfill(3),round(epoch_loss,4), round(epoch_accuracy,4)))
  
  losses.append(epoch_loss)
  accuracies.append(epoch_accuracy)

# 테스트데이터 학습하기
output = net(X_test)
print(torch.max(output, dim=1))
_, predicted = torch.max(output, dim=1)
accuracy = round((predicted == y_test).sum().item() / len(y_test),4)


print("test_set accuracy :", round(accuracy,4))
```
